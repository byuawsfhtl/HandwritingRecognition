# Train Configuration File

# Optional path to tar with images, speeds up loading the dataset as a single file when large
image_tar_path:
# The path to a tab-delimited CSV file containing training data information formatted as | IMG_PATH | Transcription |
train_csv_path: ./data/example/labels.csv
# The path to a tab-delimited CSV file containing validation data information formatted as | IMG_PATH | Transcription |
val_csv_path:
# The ratio used to determine the size of the train/validation split. If split_train_size is set to 0.8, then the
# training set will contain 80% of the data, and validation 20%. The dataset is not shuffled before being split.
split_train_size: 0.8

# Whether or not to apply the noise augmentation to the training dataset
apply_noise_augmentation: False
# Whether or not to apply the bleedthrough augmentation to the training dataset
apply_bleedthrough_augmentation: False
# Whether or not to apply the grid warp augmentation to the training dataset
apply_grid_warp_augmentation: True
# The interval in pixels between control points in the grid warp augmentation
grid_warp_interval: 16
# The standard deviation required in the grid warp augmentation
grid_warp_stddev: 2

# The path to store the trained model weights
model_out: ./data/model_weights/example_model/run1
# The path to pre-trained model weights
model_in:

# The number of epochs to train
epochs: 100
# The number of images in a mini-batch
batch_size: 10
# The learning rate the optimizer uses during training
learning_rate: 0.0003  # DO NOT use scientific notation! Pyyaml interprets scientific notation as string.
# The max number of characters in a line-level transcription
max_seq_size: 128
# The size which all images will be resized - (height, width)
img_size: (64, 1024)

# String including the character set for the model (charset: abcd1234) If no characters are specified, the default is used.
charset: