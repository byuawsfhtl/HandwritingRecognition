# The path to a tab-delimited CSV file containing training data information formatted as | IMG_PATH | Transcription |
csv_path: ./data/example/labels.csv
# How much of the dataset should be used when testing and acquiring error rates
dataset_eval_size: 0.005
# Specify the recognition architecture. Available: ['flor', 'gtr']
recognition_architecture: flor
# If using the gtr architecture, you can specify the number of filters here for each standard gateblock
std_gateblock_filters: 512
# If using the gtr architecture, you can specify the number of filters here for each pooling gateblock
pooling_gateblock_filters: 128
# If using the gtr architecture, you can specify the number of gateblocks required in the architecture
num_gateblocks: 7
# If using the gtr architecture, you can specify at which height the feature map must be when it is collapsed by avg pooling
avg_pool_height: 2
# The path to the pre-trained model weights
model_in: ./data/model_weights/testrun
# The number of images in a mini-batch
batch_size: 2
# The max number of characters in a line-level transcription
max_seq_size: 128
# The size which all images will be resized - (height, width)
img_size: (64, 1024)

# String including the character set for the model (charset: abcd1234) If no characters are specified, the default is used.
charset:
# Whether or not to print examples to the console that contains predictions (using bp and wbs) and ground-truth labels
show_predictions: True

# Non-Punctuation character set (wbs_word_charset: '12345'). If not characters are specified, the default is used.
wbs_punctuation:
# Beam width use for wbs algorithm
wbs_beam_width: 15

